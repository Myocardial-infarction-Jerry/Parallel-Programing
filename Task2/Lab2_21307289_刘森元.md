# Parallel-Programming Task2

<center><div style='height:2mm;'></div><div style="font-family:华文楷体;font-size:14pt;">刘森元, 21307289</div></center>
<center><span style="font-family:华文楷体;font-size:9pt;line-height:9mm">中山大学计算机学院</span>
</center>

Codes on https://github.com/Myocardial-infarction-Jerry/Parallel-Programming/tree/main/Task2.

Project built by CMake.

```shell
> cd Task2
> cmake . && make
> mpirun -np 8 MPIMatMul	// Customize running
```

## Environment

11th Gen Intel(R) Core(TM) i7-11700KF @ 3.60GHz

NVIDIA GeForce RTX 3080 Ti O12G

Windows Subsystem for Linux @ Ubuntu 22.04 LTS

## Task

改进上次实验中的 MPI 并行矩阵乘法 (MPI-v1), 并讨论不同通信方式对性能的影响.

> [!NOTE]
>
> ***输入:*** $m,n,k$ 三个整数, 每个整数的取值范围均为 $[128,2048]$.
>
> ***问题描述:*** 随机生成 $m\times n$ 的矩阵 $A$ 以及 $n\times k$ 的矩阵 $B$, 并对这两个矩阵进行矩阵乘法运算, 得到矩阵 $C$.
>
> ***输出:*** $A,B,C$ 三个矩阵, 及矩阵计算所消耗的时间 $t$.
>
> ***要求:***
>
> 1. 采用 MPI 集合通信实现并行矩阵乘法中的进程间通信; 使用 `MPI_Typecreate_struct` 聚合 MPI 进程内变量后进行通信; 尝试不同数据/任务划分方式 (选做).
> 2. 对于不同实现方式, 调整并记录不同线程数量 (1-16) 及矩阵规模 (128-2048) 下的时间开销, 填写下页表格, 并分析其性能及扩展性.

## Theory

由于矩阵的可分性, 使用并行通用矩阵乘法的朴素思想, 即将矩阵分割为 $size$ 等份, 在多个线程中进行计算, 最后由 master 核进行拼接.

![image-20240325163040831](/Users/qiu_nangong/Library/Application Support/typora-user-images/image-20240325163040831.png)

如上图的逐行计算, 对于 $m\times n$ 的矩阵 $A$, 可将其切分为若干个 $subM\times n$ 子矩阵分别与矩阵 $B$ 相乘, 最后进行拼接, 其中 $subM=(m+size-1)/size$, 保证均分.

## Code

> [!CAUTION]
>
> 源代码详见 [***MPIMatMul.cpp***](https://github.com/Myocardial-infarction-Jerry/Parallel-Programming/blob/main/Task2/MPIMatMul.cpp).

### 朴素矩阵乘法

```c++
void matMul(float *A, float *B, float *C, int m, int n, int k) {
    for (int i = 0; i < m; i++) {
        for (int j = 0; j < k; j++) {
            C[i * k + j] = 0;
            for (int l = 0; l < n; l++) {
                C[i * k + j] += A[i * n + l] * B[l * k + j];
            }
        }
    }
}
```

### 进行矩阵划分

```cpp
// Set the matrix dimensions
std::cin >> M >> n >> k;
std::cerr << "Calculating for m = " << M << ", n = " << n << ", k = " << k << std::endl;
std::cerr << "Running on " << size << " processes\n";

// Calculate the submatrix size for each process
m = (M + size - 1) / size;
std::cerr << "subM = " << m << std::endl;
```

对于矩阵使用零补全的方式保证整除.

### 数据聚合及传输

```cpp
MPI_Datatype matrixSizeType;
MPI_Type_contiguous(3, MPI_INT, &matrixSizeType);
MPI_Type_commit(&matrixSizeType);

MatrixSize matrixSize;
if (rank == MASTER_RANK) {
	...
	
    // Set the dimensions of the matrixSize struct
    matrixSize.m = m;
    matrixSize.n = n;
    matrixSize.k = k;
}

auto startTime = MPI_Wtime();

// Broadcast the matrixSize struct to all processes
MPI_Bcast(&matrixSize, 1, matrixSizeType, MASTER_RANK, MPI_COMM_WORLD);

// Update the local dimensions based on the received matrixSize
m = matrixSize.m;
n = matrixSize.n;
k = matrixSize.k;

// Allocate memory for the submatrices
float *subA = new float[m * n];
B = new float[n * k];
float *subC = new float[m * k];

// Scatter matrix A to all processes
MPI_Scatter(A, m * n, MPI_FLOAT, subA, m * n, MPI_FLOAT, MASTER_RANK, MPI_COMM_WORLD);

// Broadcast matrix B to all processes
MPI_Bcast(B, n * k, MPI_FLOAT, MASTER_RANK, MPI_COMM_WORLD);

// Perform matrix multiplication on the submatrices
matMul(subA, B, subC, m, n, k);

// Gather the submatrices C from all processes to the master process
MPI_Gather(subC, m * k, MPI_FLOAT, C, m * k, MPI_FLOAT, MASTER_RANK, MPI_COMM_WORLD);

auto endTime = MPI_Wtime();
```

## Result

以 8 核心 512 矩阵为例

![image-20240401185909921](/Users/qiu_nangong/Library/Application Support/typora-user-images/image-20240401185909921.png)

其中 `matMul time` 为并行矩阵乘法运行时间, `Running time` 为串行矩阵乘法运行时间.

| 进程数量/矩阵规模 (s) | 128        | 256        | 512       | 1024     | 2048    |
| --------------------- | ---------- | ---------- | --------- | -------- | ------- |
| 1                     | 0.00571813 | 0.0442355  | 0.378732  | 3.55125  | 30.4362 |
| 2                     | 0.00325512 | 0.0249743  | 0.267912  | 2.40782  | 28.9294 |
| 4                     | 0.00171017 | 0.0205275  | 0.14179   | 1.54117  | 20.3192 |
| 8                     | 0.00107397 | 0.012676   | 0.114885  | 0.894698 | 12.0781 |
| 16                    | 0.00573091 | 0.00768703 | 0.0707892 | 0.645633 | 12.8843 |

- 随着进程数量的增加, 程序的运行时间在大多数情况下都有所减少. 
- 当进程数量增加到 16 时, 对于规模为 2048 的矩阵, 运行时间并没有显著减少. 这是因为进程间的通信开销开始超过了并行计算带来的收益, 即 Amdahl 定律. 
- 随着矩阵规模的增加, 程序的运行时间也在增加. 矩阵乘法的计算复杂度为 $O(n^3)$, 所以当矩阵规模增加时, 所需的计算量也会显著增加. 
- 程序在多进程下表现出了良好的性能提升, 但当进程数量增加到一定程度后, 通信开销可能会开始影响性能. 
